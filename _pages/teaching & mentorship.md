---
layout: archive
title: 
permalink: /teaching_mentorship/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Teaching
======

* **Mechanisms & Robotics (ME G511)** : August 2022 - December 2022 \
    Birla Institute of Technology and Science, Pilani \
    Participated in the evaluation and development of assignments based on robot kinematics, dynamics and controls in
    collaboration with a [Prof. Bijay Kumar Rout](https://www.bits-pilani.ac.in/pilani/teaching/?faculty=bijay-k-routphd).

---

Mentorship
======

Worked with 5 undergraduate students and a post doc as part of their internship/course project, related to robot skill development for the robotic manipulator. A few of the projects are :

* **Human Modelling from RGBD Sensors for Robotic Applications** : May 2024 - July 2024 \
    *Tanveer Singh, Undergraduate Student (IIT-Ropar)*\
In assistive technology, modeling the human(s) to be assisted in the robot's world model is crucial for smooth task execution. This includes recognizing the human's identity, 3D location, posture, and commands. This project explores human modeling using RGB-D sensors, integrating object detection models like YOLO for human detection and tracking, and face recognition models like SFace for identifying individuals. We propose a method to simultaneously detect, recognize, and track humans in dynamic environments using a ROS-based pipeline. This pipeline labels tracked individuals, calculates their 3D location, and localizes the speaking person with a ReSpeaker Mic Array for task execution based on voice commands. [Report](https://drive.google.com/file/d/1cOxQ_YaNejphPoZd9czq340CaRp3hRph/view?usp=sharing)

* **Automatic Extraction of Semantic Information from Visual+3D Data for Intelligent** : May 2024 - July 2024 \
    *Pushpendra Jain, Undergraduate Student (IIT-Ropar)*\
This project focuses on developing a system for the automatic extraction of semantic information from visual and 3D data, specifically designed to enhance the functionality of intelligent robots. The system is tailored to improve robotic performance in low-light environments, which is critical for applications such as defense, surveillance, and exploration. By leveraging ground-based point cloud data, the project ensures high reliability and accuracy in real-world scenarios. Key objectives include segmenting different object classes from point cloud data and benchmarking the effectiveness of 3D segmentation models. [Report](https://drive.google.com/file/d/1RjMg3org7P2hqeDd1RstbnUpBVgWAEyp/view?usp=sharing)

* **Static and Dynamic Gesture Instructed Robot Task Execution** \
    *Shubhangi Nema, Post Doc (IIT-Delhi)*\
Gestures provide non-verbal cues that complement spoken instructions, enhancing the robotâ€™s understanding of user intent. This work leverages multi-modal input fusion, combining visual data from an image frame, gesture signals, and natural language commands to enable robots to perform complex tasks. The Vision-Language Model (VLM) processes these inputs, identifying relevant objects and interpreting gestures within the context of the language command. This object detection serves as the foundation for generating robot actions through a policy that maps the current state and goal to specific movements. By incorporating human gestures and language into its decision-making process, the system allows for seamless human-robot collaboration, enhancing robot responsiveness and task execution in real-world environments.